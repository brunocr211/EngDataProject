{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration Data\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to use techniques learned in the Udacity Nanodegree course on Data Engineer to create a fact and dimension tables regarding US immigration data, together with analysis on temperature and airport characteristics. Here we will use two formats of data (SAS and .csv) to build FACT and dimension tables, splitting the US immigration dataset in regards to what are its informations about (flight? passenger?), and add informtion on temperature on the dates observed.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F  \n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.queries_new import immigration_insert, temperature_insert, passenger_insert, \\\n",
    "time_insert, status_insert\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Spark\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "                            .master(\"local[*]\")\\\n",
    "                            .enableHiveSupport()\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Immigration dataset\n",
    "df_immigration = spark.read.parquet('sas_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1: I94 Immigration Data from the US National Tourism and Trade Office \n",
    "\n",
    "- Is the data that will be used to build our FACT table. It contains information on airline, visatipe, destination of people immigrating to the US.\n",
    "- Firstly we will check for missing data, check if the data is behaving as expected and transform the data where necessary.\n",
    "- This dataset has around 3 million lines and 28 columns. We will limit the columns to those we are interested in and explore how to break it into new tables when relevant\n",
    "- This data is updated monthly\n",
    "- For this project I opted for deleting columns with more than 30% of its values being null. However, it is expected that cases with null values happen in data engineering projects, and this is one of many possible approaches.\n",
    "- In this dataset we have a lot of infomations, we can split it in regards to what the informations are about.\n",
    "- We make use of draw.io to build visualization for the schema\n",
    "- I will allow nan values to continue as nan, without replacement techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2:Temperature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technologies and Tools \n",
    "\n",
    "Here I opted for using pyspark and take a vantage of parallem processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|5748522.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20579.0|  57.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1959.0|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|5748523.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  66.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|5748524.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  41.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|5748525.0|2016.0|   4.0| 245.0| 464.0|    HOU|20574.0|    1.0|     FL|20581.0|  27.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1989.0|10292016|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|5748526.0|2016.0|   4.0| 245.0| 464.0|    LOS|20574.0|    1.0|     CA|20581.0|  26.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1990.0|10292016|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "|5748527.0|2016.0|   4.0| 245.0| 504.0|    NEW|20574.0|    1.0|     MA|20576.0|  44.0|    2.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1972.0|10292016|     M|  null|     UA|9.493828593E10|01215|      B2|\n",
      "|5748528.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|   null|20575.0|  39.0|    2.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1977.0|10292016|     M|  null|     CM|9.501810463E10|00472|      B2|\n",
      "|5748529.0|2016.0|   4.0| 245.0| 504.0|    WAS|20574.0|    1.0|     VA|20596.0|  38.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1978.0|10292016|     M|  null|     CM|9.492489983E10|00488|      B2|\n",
      "|5748530.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|     CA|20577.0|  56.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1960.0|10292016|     F|  null|     CM|9.492648103E10|00302|      B2|\n",
      "|5748531.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|     CA|20577.0|  38.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1978.0|10282016|     M|  null|     CM|9.492629303E10|00302|      B2|\n",
      "|5748532.0|2016.0|   4.0| 245.0| 504.0|    MIA|20574.0|    1.0|     FL|20581.0|  53.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1963.0|10292016|     F|  null|     CM|9.500640513E10|00430|      B2|\n",
      "|5748534.0|2016.0|   4.0| 245.0| 528.0|    SFR|20574.0|    1.0|     CA|   null|  84.0|    2.0|  1.0|20160430|     HNK| null|      G|   null|   null|   null| 1932.0|10282016|     F|  null|     CX|9.492476223E10|00872|      B2|\n",
      "|5748876.0|2016.0|   4.0| 245.0| 582.0|    HOU|20574.0|    1.0|     TX|20583.0|  43.0|    1.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1973.0|10292016|     M|  null|     UA|9.499463063E10|05574|      B1|\n",
      "|5748877.0|2016.0|   4.0| 245.0| 582.0|    HOU|20574.0|    1.0|     TX|20583.0|  30.0|    1.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1986.0|10292016|     F|  null|     UA|9.499447663E10|05574|      B1|\n",
      "|5748881.0|2016.0|   4.0| 245.0| 582.0|    LOS|20574.0|    1.0|     CA|20575.0|  34.0|    2.0|  1.0|20160430|     SHG| null|      G|      O|   null|      M| 1982.0|10292016|     M|  null|     AM|9.496770903E10|00646|      B2|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print to have an overview\n",
    "df_immigration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create udf to convert SAS date to PySpark date \n",
    "def convert(x):\n",
    "    if x:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(x)).isoformat()\n",
    "    return None\n",
    "    \n",
    "\n",
    "convertUDF = udf(lambda x: convert(x), StringType())\n",
    "\n",
    "df_immigration = df_immigration.withColumn(\"arrdate\", convertUDF(df_immigration.arrdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking if we need to change the datatype or if it corresponds to our desired type\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------\n",
      " cicid    | 3096313 \n",
      " i94yr    | 1       \n",
      " i94mon   | 1       \n",
      " i94cit   | 243     \n",
      " i94res   | 229     \n",
      " i94port  | 299     \n",
      " arrdate  | 30      \n",
      " i94mode  | 4       \n",
      " i94addr  | 457     \n",
      " depdate  | 235     \n",
      " i94bir   | 112     \n",
      " i94visa  | 3       \n",
      " count    | 1       \n",
      " dtadfile | 117     \n",
      " visapost | 530     \n",
      " occup    | 111     \n",
      " entdepa  | 13      \n",
      " entdepd  | 12      \n",
      " entdepu  | 2       \n",
      " matflag  | 1       \n",
      " biryear  | 112     \n",
      " dtaddto  | 777     \n",
      " gender   | 4       \n",
      " insnum   | 1913    \n",
      " airline  | 534     \n",
      " admnum   | 3075579 \n",
      " fltno    | 7152    \n",
      " visatype | 17      \n",
      "\n",
      "CPU times: user 32.8 ms, sys: 19 ms, total: 51.8 ms\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Checking how many different values we have for each column and other statistics\n",
    "df_immigration.select(*(F.countDistinct(F.col(c)).alias(c) for c in df_immigration.columns)).show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We only have data on year 2016 month 04\n",
    "- 243 different cities\n",
    "- 534 different airline values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GA',\n",
       " 'R0E',\n",
       " 'FC',\n",
       " 'BSK',\n",
       " 'NK',\n",
       " 'DE',\n",
       " 'BS',\n",
       " 'MP',\n",
       " 'M0M',\n",
       " 'UX',\n",
       " 'LE',\n",
       " 'B01',\n",
       " 'G5A',\n",
       " 'K4',\n",
       " 'Q5U',\n",
       " '3',\n",
       " 'T8B',\n",
       " '9V',\n",
       " '001',\n",
       " 'D2V',\n",
       " 'LU',\n",
       " 'XD',\n",
       " 'CL',\n",
       " 'EY',\n",
       " 'K8',\n",
       " '9K',\n",
       " 'Q2',\n",
       " 'TN',\n",
       " 'YL',\n",
       " '734',\n",
       " '*GA',\n",
       " 'IG',\n",
       " 'G5J',\n",
       " 'EI',\n",
       " '2U',\n",
       " '5X',\n",
       " '79A',\n",
       " 'DL',\n",
       " '3M',\n",
       " 'LO',\n",
       " 'AZ',\n",
       " 'AS',\n",
       " 'VW',\n",
       " 'IB',\n",
       " 'SJ',\n",
       " 'B73',\n",
       " 'A3R',\n",
       " '35L',\n",
       " '0ZD',\n",
       " 'PY',\n",
       " 'PS',\n",
       " 'G3',\n",
       " '278',\n",
       " 'CV',\n",
       " 'VX',\n",
       " 'TC',\n",
       " 'ZW',\n",
       " 'MU',\n",
       " '0FR',\n",
       " 'TOS',\n",
       " '99',\n",
       " 'AD',\n",
       " '5Y',\n",
       " 'E1Q',\n",
       " 'T4A',\n",
       " 'DZ',\n",
       " 'SN',\n",
       " 'Q0P',\n",
       " 'ON',\n",
       " 'KR',\n",
       " 'AJ',\n",
       " 'IJW',\n",
       " '605',\n",
       " 'ARU',\n",
       " 'PZ',\n",
       " '0FF',\n",
       " 'BB',\n",
       " '537',\n",
       " '6R',\n",
       " 'TOM',\n",
       " 'EL',\n",
       " 'RJD',\n",
       " 'H0P',\n",
       " 'G1A',\n",
       " '813',\n",
       " '18',\n",
       " 'EW',\n",
       " 'TIW',\n",
       " 'Y6Y',\n",
       " 'B3D',\n",
       " 'OO',\n",
       " 'PP',\n",
       " 'V6D',\n",
       " 'C5X',\n",
       " '3X',\n",
       " '351',\n",
       " 'ET',\n",
       " 'B2H',\n",
       " 'A1B',\n",
       " '848',\n",
       " 'PL',\n",
       " 'C7',\n",
       " 'FI',\n",
       " 'WU',\n",
       " 'BC',\n",
       " '0L7',\n",
       " 'CSQ',\n",
       " 'P8K',\n",
       " 'LP',\n",
       " 'LY',\n",
       " 'CH',\n",
       " 'KQ',\n",
       " 'T0',\n",
       " '031',\n",
       " 'HA',\n",
       " 'EA',\n",
       " 'HB',\n",
       " '0AZ',\n",
       " 'LM',\n",
       " 'I0H',\n",
       " '35T',\n",
       " 'YCK',\n",
       " 'N9',\n",
       " 'TYW',\n",
       " 'E8D',\n",
       " '15',\n",
       " 'WK',\n",
       " 'QT',\n",
       " '0AF',\n",
       " '81',\n",
       " 'D0S',\n",
       " 'NJ',\n",
       " 'W4J',\n",
       " 'LA',\n",
       " 'B57',\n",
       " 'WR',\n",
       " 'AR',\n",
       " 'ZX',\n",
       " '2D',\n",
       " 'LN',\n",
       " 'JE',\n",
       " 'DCS',\n",
       " 'P2P',\n",
       " 'TM',\n",
       " '0MY',\n",
       " '969',\n",
       " 'AV',\n",
       " 'CA',\n",
       " '552',\n",
       " 'GB',\n",
       " 'B0E',\n",
       " 'KLM',\n",
       " '516',\n",
       " 'VS',\n",
       " 'V4',\n",
       " '7C',\n",
       " 'L9M',\n",
       " 'L7C',\n",
       " 'M7',\n",
       " 'US',\n",
       " 'TW',\n",
       " 'OR',\n",
       " 'YE',\n",
       " 'HPJ',\n",
       " '692',\n",
       " 'WS',\n",
       " 'UU',\n",
       " 'BX',\n",
       " 'U8S',\n",
       " 'SK',\n",
       " 'U1H',\n",
       " 'JBU',\n",
       " 'C5Q',\n",
       " '610',\n",
       " 'KJ',\n",
       " '899',\n",
       " '850',\n",
       " 'N7',\n",
       " 'T9U',\n",
       " 'S5',\n",
       " 'OA',\n",
       " 'NQ',\n",
       " 'KI',\n",
       " 'ISC',\n",
       " 'M5',\n",
       " 'DO',\n",
       " 'BO',\n",
       " 'D5',\n",
       " 'UC',\n",
       " '663',\n",
       " 'AH',\n",
       " 'TG',\n",
       " '5T',\n",
       " 'OH',\n",
       " 'C1',\n",
       " 'JB',\n",
       " 'JR',\n",
       " 'N5',\n",
       " 'C0B',\n",
       " 'YV',\n",
       " '348',\n",
       " '13',\n",
       " '006',\n",
       " '10A',\n",
       " 'CSN',\n",
       " 'LR',\n",
       " '88I',\n",
       " 'MQ',\n",
       " 'AB',\n",
       " 'L7L',\n",
       " 'S3',\n",
       " 'TE',\n",
       " 'MS',\n",
       " 'AL',\n",
       " 'CE',\n",
       " 'Y9T',\n",
       " 'ZZ',\n",
       " 'LD',\n",
       " 'DI',\n",
       " 'SV',\n",
       " '4O',\n",
       " 'AA',\n",
       " 'CC',\n",
       " 'LK',\n",
       " 'GT',\n",
       " 'QX',\n",
       " '222',\n",
       " 'WJ',\n",
       " 'YFA',\n",
       " 'ZE',\n",
       " 'AX',\n",
       " 'AI',\n",
       " 'G8L',\n",
       " 'BVR',\n",
       " 'ZI',\n",
       " 'BA',\n",
       " 'VCN',\n",
       " 'SU',\n",
       " '802',\n",
       " 'C8',\n",
       " '49B',\n",
       " '447',\n",
       " 'X2P',\n",
       " '7Z',\n",
       " 'AT',\n",
       " '93',\n",
       " '549',\n",
       " 'YI',\n",
       " 'KX',\n",
       " 'TUS',\n",
       " '3V',\n",
       " '874',\n",
       " '078',\n",
       " 'YPT',\n",
       " 'QF',\n",
       " 'CHP',\n",
       " 'SQ',\n",
       " 'N3V',\n",
       " '*FF',\n",
       " 'LZ',\n",
       " 'SB',\n",
       " 'N6Y',\n",
       " '366',\n",
       " '41C',\n",
       " '76',\n",
       " '743',\n",
       " 'DR',\n",
       " 'FL',\n",
       " 'YAF',\n",
       " '029',\n",
       " 'DYA',\n",
       " 'NCB',\n",
       " 'HC',\n",
       " 'EV',\n",
       " 'JQ',\n",
       " '0UE',\n",
       " 'YC',\n",
       " 'CM',\n",
       " 'JJ',\n",
       " 'CI',\n",
       " 'OX',\n",
       " 'SWQ',\n",
       " 'DT',\n",
       " 'Z6T',\n",
       " 'PD',\n",
       " 'U7R',\n",
       " '8I',\n",
       " 'B1M',\n",
       " 'KL',\n",
       " 'KE',\n",
       " '789',\n",
       " 'T1G',\n",
       " 'ATN',\n",
       " 'QR',\n",
       " 'B6',\n",
       " '7V',\n",
       " '512',\n",
       " 'F0D',\n",
       " 'K6R',\n",
       " '62A',\n",
       " 'B1J',\n",
       " 'AVI',\n",
       " '810',\n",
       " '635',\n",
       " '9E',\n",
       " 'TP',\n",
       " 'B2V',\n",
       " '4A',\n",
       " 'G4D',\n",
       " '56J',\n",
       " 'AU',\n",
       " '203',\n",
       " 'KW',\n",
       " 'I2M',\n",
       " '009',\n",
       " 'YF',\n",
       " 'VI',\n",
       " '0FT',\n",
       " 'LJ',\n",
       " 'U8H',\n",
       " 'UJ',\n",
       " 'EQ',\n",
       " 'KU',\n",
       " 'EAC',\n",
       " 'OS',\n",
       " '9H',\n",
       " '843',\n",
       " 'D2U',\n",
       " 'V2',\n",
       " '85J',\n",
       " 'C9B',\n",
       " 'CX',\n",
       " 'W3',\n",
       " 'Q6Q',\n",
       " 'XA',\n",
       " 'IJ',\n",
       " 'GUY',\n",
       " 'Y7Q',\n",
       " 'D9',\n",
       " 'LG',\n",
       " '3U',\n",
       " 'G6B',\n",
       " 'W8',\n",
       " '7A',\n",
       " 'TA',\n",
       " 'TS',\n",
       " 'FYG',\n",
       " '765',\n",
       " 'XB',\n",
       " 'NRL',\n",
       " 'NH',\n",
       " 'JL',\n",
       " 'YX',\n",
       " '881',\n",
       " 'JA',\n",
       " '0BA',\n",
       " '5C',\n",
       " 'PK',\n",
       " 'IA',\n",
       " 'SE',\n",
       " '9W',\n",
       " '596',\n",
       " 'AC',\n",
       " 'LI',\n",
       " '5J',\n",
       " '742',\n",
       " 'VES',\n",
       " '1B',\n",
       " '360',\n",
       " '78B',\n",
       " 'FJ',\n",
       " 'L6K',\n",
       " 'S3T',\n",
       " 'CEY',\n",
       " 'GMT',\n",
       " 'PVO',\n",
       " 'SA',\n",
       " 'DQ',\n",
       " 'PWA',\n",
       " 'K0W',\n",
       " 'WG',\n",
       " 'TB',\n",
       " 'X3',\n",
       " 'YNT',\n",
       " 'NA',\n",
       " 'BE',\n",
       " 'UA',\n",
       " 'MC',\n",
       " 'MV',\n",
       " '365',\n",
       " 'S4',\n",
       " 'T2W',\n",
       " 'F9',\n",
       " '0MT',\n",
       " 'JK',\n",
       " 'XLA',\n",
       " 'RU',\n",
       " 'RJ',\n",
       " '020',\n",
       " 'LX',\n",
       " 'Y5R',\n",
       " 'MT',\n",
       " 'CT',\n",
       " '0FY',\n",
       " 'CZ',\n",
       " 'EC',\n",
       " 'YA',\n",
       " 'JY',\n",
       " 'CP',\n",
       " 'FLE',\n",
       " 'MX',\n",
       " 'U9Y',\n",
       " 'EU',\n",
       " '468',\n",
       " 'G7',\n",
       " 'NU',\n",
       " 'KZ',\n",
       " 'TJ',\n",
       " 'M3G',\n",
       " 'Q7',\n",
       " 'FA',\n",
       " '459',\n",
       " 'OZ',\n",
       " 'IZ',\n",
       " 'PR',\n",
       " '661',\n",
       " '134',\n",
       " '*UU',\n",
       " '7I',\n",
       " 'WQ',\n",
       " '239',\n",
       " 'LL',\n",
       " 'TK',\n",
       " '9M',\n",
       " 'GMA',\n",
       " 'UN',\n",
       " 'NJE',\n",
       " 'UT',\n",
       " '27A',\n",
       " 'DPJ',\n",
       " 'RS',\n",
       " 'U0C',\n",
       " 'UP',\n",
       " 'UK',\n",
       " 'OB',\n",
       " 'AM',\n",
       " 'LH',\n",
       " 'E9',\n",
       " '026',\n",
       " 'HU',\n",
       " 'QK',\n",
       " 'DJT',\n",
       " 'OI',\n",
       " '545',\n",
       " 'OJ',\n",
       " '887',\n",
       " '98J',\n",
       " 'N8',\n",
       " '3R',\n",
       " '0HC',\n",
       " 'OY',\n",
       " 'HY',\n",
       " 'JO',\n",
       " 'UZ',\n",
       " 'ESF',\n",
       " '2V',\n",
       " 'TL',\n",
       " '83J',\n",
       " 'EK',\n",
       " 'MW',\n",
       " 'Z7M',\n",
       " 'N6',\n",
       " 'OF',\n",
       " 'XL',\n",
       " 'B1',\n",
       " 'XP',\n",
       " 'RV',\n",
       " '52',\n",
       " 'VIV',\n",
       " 'VA',\n",
       " '37',\n",
       " 'M0C',\n",
       " '855',\n",
       " '3S',\n",
       " 'ZP',\n",
       " 'WW',\n",
       " 'QA',\n",
       " 'J2',\n",
       " 'RX',\n",
       " 'JU',\n",
       " '4M',\n",
       " 'SW',\n",
       " 'BVI',\n",
       " 'V8S',\n",
       " '97R',\n",
       " 'Y4',\n",
       " 'AGF',\n",
       " '0N3',\n",
       " 'AF',\n",
       " 'BR',\n",
       " 'MA',\n",
       " '71',\n",
       " 'A0',\n",
       " 'WN',\n",
       " '34D',\n",
       " 'GU',\n",
       " '784',\n",
       " '803',\n",
       " 'WMA',\n",
       " '524',\n",
       " 'AY',\n",
       " 'SY',\n",
       " 'UL',\n",
       " 'G5Q',\n",
       " 'BW',\n",
       " 'LT',\n",
       " '377',\n",
       " 'VP',\n",
       " '12L',\n",
       " '0B8',\n",
       " 'DK',\n",
       " 'JS',\n",
       " 'NZ',\n",
       " '142',\n",
       " 'PHM',\n",
       " '40',\n",
       " 'D0',\n",
       " 'VR',\n",
       " 'MM',\n",
       " 'E7E',\n",
       " 'D6A',\n",
       " 'DY',\n",
       " '0J',\n",
       " '959']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do we have misspells / typos on airlines?\n",
    "df_immigration.select(F.collect_set(\"airline\").alias(\"airline\")).first()[\"airline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before remove duplicates  -> 3096313\n",
      "After remove duplicates -> 3096313\n"
     ]
    }
   ],
   "source": [
    "#are there duplicate values that would help me reduce my dataset size?\n",
    "print(f'Before remove duplicates  -> {df_immigration.count()}')\n",
    "df_immigration = df_immigration.drop_duplicates()\n",
    "print(f'After remove duplicates -> {df_immigration.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique ID \n",
    "window = Window.orderBy(F.col('cicid'))\n",
    "df_immigration = df_immigration.withColumn('person_id', F.row_number().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I will import the temperature data\n",
    "fname = 'data/df_temp.csv'\n",
    "df_temp = spark.read.option(\"header\", \"True\").option(\"charset\", \"utf-8\").csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|_c0|        dt| AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+---+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|  0|1743-11-01|              6.068|                        1.737|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  1|1743-12-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  2|1744-01-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  3|1744-02-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  4|1744-03-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  5|1744-04-01|  5.787999999999999|                        3.624|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  6|1744-05-01|             10.644|                        1.283|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  7|1744-06-01| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  8|1744-07-01|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  9|1744-08-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 10|1744-09-01|             12.781|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 11|1744-10-01|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 12|1744-11-01| 4.6389999999999985|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 13|1744-12-01|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 14|1745-01-01|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 15|1745-02-01|-2.7319999999999998|                        1.358|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 16|1745-03-01|              0.129|                        1.088|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 17|1745-04-01|              4.042|                        1.138|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 18|1745-05-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "| 19|1745-06-01|               null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+---+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename first column to \"id\"\n",
    "df_temp = df_temp.withColumnRenamed('_c0', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data types\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Temperature to double type\n",
    "df_temp = df_temp.withColumn('AverageTemperature', F.col('AverageTemperature').cast(DoubleType())) \\\n",
    "                 .withColumn('AverageTemperatureUncertainty', F.col('AverageTemperatureUncertainty').cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data types\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we are using immigration data in the US, we can limit country here in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "| id|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+---+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|  0|1743-11-01|             6.068|                        1.737|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  1|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|  2|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+---+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limiting only to the United States\n",
    "df_temp = df_temp.filter(df_temp.Country == 'United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|      Country| count|\n",
      "+-------------+------+\n",
      "|United States|687289|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking filters result\n",
    "df_temp.groupBy('Country').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------\n",
      " id                            | 0.0                  \n",
      " dt                            | 0.0                  \n",
      " AverageTemperature            | 0.037487869004161394 \n",
      " AverageTemperatureUncertainty | 0.037487869004161394 \n",
      " City                          | 0.0                  \n",
      " Country                       | 0.0                  \n",
      " Latitude                      | 0.0                  \n",
      " Longitude                     | 0.0                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking null values \n",
    "df_temp.select(*(F.sum(F.col(c).isNull().cast(\"int\") / df_temp.count()).alias(c) for c in df_temp.columns)).show(vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of valid ports\n",
    "i94_sas_label_descriptions = \"data/I94_SAS_Labels_Descriptions.SAS\"\n",
    "\n",
    "with open(i94_sas_label_descriptions) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "result_ports = {}\n",
    "for line in lines[302:961]:\n",
    "    results = re.compile(r\"\\'(.*)\\'.*\\'(.*)\\'\").search(line)\n",
    "    result_ports[results.group(1)] = results.group(2)\n",
    "\n",
    "# Create udf to convert city to port \n",
    "def convert_city_to_port(city):\n",
    "    for key in result_ports:\n",
    "        if city.lower() in valid_ports[key].lower():\n",
    "            return key\n",
    "\n",
    "convertCity = udf(lambda x: city_to_port(x), StringType())\n",
    "\n",
    "df_temp =  df_temp \\\n",
    "    .withColumn(\"i94port\", convertCity(F.col(\"city\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before remove duplicates -> 687289\n",
      "After remove duplicates -> 687289\n"
     ]
    }
   ],
   "source": [
    "# There are duplicates values?\n",
    "print(f'Before remove duplicates -> {df_temp.count()}')\n",
    "df_temp = df_temp.drop_duplicates()\n",
    "print(f'After remove duplicates -> {df_temp.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to load airport data\n",
    "df_airport = spark.read.option(\"header\", \"True\").option(\"charset\", \"utf-8\").csv('data/i94portCodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+\n",
      "|code|            location|state|\n",
      "+----+--------------------+-----+\n",
      "| ALC|               ALCAN|   AK|\n",
      "| ANC|           ANCHORAGE|   AK|\n",
      "| BAR|BAKER AAF - BAKER...|   AK|\n",
      "| DAC|       DALTONS CACHE|   AK|\n",
      "| PIZ|DEW STATION PT LA...|   AK|\n",
      "| DTH|        DUTCH HARBOR|   AK|\n",
      "| EGL|               EAGLE|   AK|\n",
      "| FRB|           FAIRBANKS|   AK|\n",
      "| HOM|               HOMER|   AK|\n",
      "| HYD|               HYDER|   AK|\n",
      "| JUN|              JUNEAU|   AK|\n",
      "| 5KE|           KETCHIKAN|   AK|\n",
      "| KET|           KETCHIKAN|   AK|\n",
      "| MOS|MOSES POINT INTER...|   AK|\n",
      "| NIK|             NIKISKI|   AK|\n",
      "| NOM|                 NOM|   AK|\n",
      "| PKC|         POKER CREEK|   AK|\n",
      "| ORI|      PORT LIONS SPB|   AK|\n",
      "| SKA|             SKAGWAY|   AK|\n",
      "| SNP|     ST. PAUL ISLAND|   AK|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowerin column names for both datasets\n",
    "df_temp = df_temp.select([F.col(x).alias(x.lower()) for x in df_temp.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration = df_immigration.select([F.col(x).alias(x.lower()) for x in df_immigration.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport.select([F.col(x).alias(x.lower()) for x in df_airport.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking results -> ['id', 'dt', 'averagetemperature', 'averagetemperatureuncertainty', 'city', 'country', 'latitude', 'longitude', 'i94port']\n",
      "Checking results -> ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline', 'admnum', 'fltno', 'visatype', 'person_id']\n",
      "Checking results -> ['code', 'location', 'state']\n"
     ]
    }
   ],
   "source": [
    "print(f'Checking results -> {df_temp.columns}')\n",
    "print(f'Checking results -> {df_immigration.columns}')\n",
    "print(f'Checking results -> {df_airport.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model\n",
    "\n",
    "- For this project I opted for a simpler version (less dimensions tables than possible), in order to be more goal-oriented and improve time management.\n",
    "- Out fact table contains all ids used in the dimension tables\n",
    "- We have a reduced number of three dim tables, which are in regards of time, status, passengers data and temperature per city in the US.\n",
    "\n",
    "![alt_text](new_datamodel.PNG)\n",
    "\n",
    "# Pipeline Data\n",
    "\n",
    "- Graun declaration: what we want the fact table to show.\n",
    "- Identify dimensions: how we can split the data without losing a way to link them. Also, sort it by different 'themes'\n",
    "- Load and clab the datasets\n",
    "- Create fact and dim tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can find all ports in dim_airpot\n",
    "\n",
    "def create_immigration(df_immigration=df_immigration):\n",
    "\n",
    "    return df_immigration.select(\n",
    "    F.col('cicid'),\n",
    "    F.col('person_id').alias('personid'),\n",
    "    F.col('arrdate').alias('timeid'),\n",
    "    F.col('i94cit').alias('cit'),\n",
    "    F.col('i94res').alias('res'),\n",
    "    F.col('i94port').alias('port'),\n",
    "    F.col('arrdate').alias('arrdate'),\n",
    "    F.col('i94mode').alias('mode'),\n",
    "    F.col('i94addr').alias('addr'),\n",
    "    F.col('depdate'),\n",
    "    F.col('i94bir').alias('bir'),\n",
    "    F.col('i94visa').alias('visa'),\n",
    "    F.col('count'),\n",
    "    F.col('dtadfile'),\n",
    "    F.col('entdepa'),\n",
    "    F.col('entdepd'),\n",
    "    F.col('entdepu'),\n",
    "    F.col('matflag'),\n",
    "    F.col('dtaddto'),\n",
    "    F.col('airline'),\n",
    "    F.col('admnum'),\n",
    "    F.col('fltno'),\n",
    "    F.col('visatype')).createOrReplaceTempView(\"immigration\"), spark.sql(\"create table fact_immigration as select * from immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have to create the fact model on Hive from a tempview\n",
    "df_immigration.select(\n",
    "\n",
    "    F.col('cicid'),\n",
    "    F.col('person_id').alias('personid'),\n",
    "    F.col('arrdate').alias('timeid'),\n",
    "    F.col('i94cit').alias('cit'),\n",
    "    F.col('i94res').alias('res'),\n",
    "    F.col('i94port').alias('port'),\n",
    "    F.col('arrdate').alias('arrdate'),\n",
    "    F.col('i94mode').alias('mode'),\n",
    "    F.col('i94addr').alias('addr'),\n",
    "    F.col('depdate'),\n",
    "    F.col('i94bir').alias('bir'),\n",
    "    F.col('i94visa').alias('visa'),\n",
    "    F.col('count'),\n",
    "    F.col('dtadfile'),\n",
    "    F.col('entdepa'),\n",
    "    F.col('entdepd'),\n",
    "    F.col('entdepu'),\n",
    "    F.col('matflag'),\n",
    "    F.col('dtaddto'),\n",
    "    F.col('airline'),\n",
    "    F.col('admnum'),\n",
    "    F.col('fltno'),\n",
    "    F.col('visatype')\n",
    "\n",
    ").createOrReplaceTempView(\"immigration\")\n",
    "\n",
    "# Verificar quem é o person id\n",
    "\n",
    "spark.sql(\"create table fact_immigration as select * from immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+-----+-----+----+----------+----+----+-------+----+----+-----+--------+-------+-------+-------+-------+--------+-------+-------------+-----+--------+\n",
      "|cicid|personid|    timeid|  cit|  res|port|   arrdate|mode|addr|depdate| bir|visa|count|dtadfile|entdepa|entdepd|entdepu|matflag| dtaddto|airline|       admnum|fltno|visatype|\n",
      "+-----+--------+----------+-----+-----+----+----------+----+----+-------+----+----+-----+--------+-------+-------+-------+-------+--------+-------+-------------+-----+--------+\n",
      "|  6.0|       1|2016-04-29|692.0|692.0| XXX|2016-04-29|null|null|   null|37.0| 2.0|  1.0|    null|      T|   null|      U|   null|10282016|   null|1.897628485E9| null|      B2|\n",
      "+-----+--------+----------+-----+-----+----+----------+----+----+-------+----+----+-----+--------+-------+-------+-------+-------+--------+-------+-------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the fact table\n",
    "spark.sql('SELECT * FROM fact_immigration').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_airport(df_airport=df_airport):\n",
    "\n",
    "    return df_airport.createOrReplaceTempView(\"airport\"), \n",
    "    spark.sql(\"create table dim_airport as select * from airport\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have to create airport dimension\n",
    "\n",
    "df_airport.createOrReplaceTempView(\"airport\")\n",
    "\n",
    "spark.sql(\"create table dim_airport as select * from airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+\n",
      "|code|            location|state|\n",
      "+----+--------------------+-----+\n",
      "| ALC|               ALCAN|   AK|\n",
      "| ANC|           ANCHORAGE|   AK|\n",
      "| BAR|BAKER AAF - BAKER...|   AK|\n",
      "| DAC|       DALTONS CACHE|   AK|\n",
      "| PIZ|DEW STATION PT LA...|   AK|\n",
      "| DTH|        DUTCH HARBOR|   AK|\n",
      "| EGL|               EAGLE|   AK|\n",
      "| FRB|           FAIRBANKS|   AK|\n",
      "| HOM|               HOMER|   AK|\n",
      "| HYD|               HYDER|   AK|\n",
      "| JUN|              JUNEAU|   AK|\n",
      "| 5KE|           KETCHIKAN|   AK|\n",
      "| KET|           KETCHIKAN|   AK|\n",
      "| MOS|MOSES POINT INTER...|   AK|\n",
      "| NIK|             NIKISKI|   AK|\n",
      "| NOM|                 NOM|   AK|\n",
      "| PKC|         POKER CREEK|   AK|\n",
      "| ORI|      PORT LIONS SPB|   AK|\n",
      "| SKA|             SKAGWAY|   AK|\n",
      "| SNP|     ST. PAUL ISLAND|   AK|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dim_airport\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp(df_temp=df_temp):\n",
    "    return  df_temp.select(\n",
    "\n",
    "    F.concat(F.split(F.col('dt'), \"-\")[0], F.lit(\"-\"), F.split(F.col('dt'), \"-\")[1]).alias('dt'),\n",
    "    F.col('i94port').alias('port'),\n",
    "    F.col('city'),\n",
    "    F.col('averagetemperature'),\n",
    "    F.col('averagetemperatureuncertainty')).groupBy('dt', 'city', 'port')\\\n",
    "        .agg(F.avg('averagetemperature').alias(\"averagetemperature\"),\\\n",
    "             F.avg('averagetemperatureuncertainty').alias(\"averagetemperatureuncertainty\"))\\\n",
    "                 .createOrReplaceTempView(\"temperature\"), spark.sql(\"create table dim_temperature as select * from temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have to create temperature dimension\n",
    "\n",
    "df_temp.select(\n",
    "\n",
    "    F.concat(F.split(F.col('dt'), \"-\")[0], F.lit(\"-\"), F.split(F.col('dt'), \"-\")[1]).alias('dt'),\n",
    "    F.col('i94port').alias('port'),\n",
    "    F.col('city'),\n",
    "    F.col('averagetemperature'),\n",
    "    F.col('averagetemperatureuncertainty')\n",
    "\n",
    ").groupBy('dt', 'city', 'port')\\\n",
    ".agg(F.avg('averagetemperature').alias(\"averagetemperature\"), F.avg('averagetemperatureuncertainty').alias(\"averagetemperatureuncertainty\"))\\\n",
    ".createOrReplaceTempView(\"temperature\")\n",
    "\n",
    "spark.sql(\"create table dim_temperature as select * from temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----+------------------+-----------------------------+\n",
      "|     dt|          city|port|averagetemperature|averagetemperatureuncertainty|\n",
      "+-------+--------------+----+------------------+-----------------------------+\n",
      "|1870-09|   Brownsville| BRO| 28.19100000000001|                        2.518|\n",
      "|1821-03|       Norfolk| NOR|7.7360000000000015|                        3.464|\n",
      "|1902-07|    Birmingham| BHX|            27.962|           0.4970000000000001|\n",
      "|1766-11|       Jackson| JAC|              null|                         null|\n",
      "|1928-12|   Springfield| SPI|1.1303333333333332|           0.4663333333333333|\n",
      "|1928-01|       Buffalo| BUF|            -4.152|                        0.223|\n",
      "|1933-04|       El Paso| ELP|            13.867|                        0.212|\n",
      "|1945-10|    Carrollton|null|            17.969|                        0.249|\n",
      "|1865-02|        Edison|null|            -3.153|           0.8240000000000001|\n",
      "|1979-11|         Flint|null|             4.438|          0.14800000000000002|\n",
      "|1924-07|     Fullerton|null|             18.32|                         0.48|\n",
      "|1924-03|         Provo|null|0.4750000000000001|                        0.363|\n",
      "|1975-04|         Provo|null|              4.05|                        0.208|\n",
      "|1821-06|    Saint Paul|null|19.862000000000002|                        4.748|\n",
      "|1843-11|         Salem|null|             4.958|                        1.955|\n",
      "|1870-09| Sunrise Manor|null|            23.838|                        0.602|\n",
      "|1932-05| Sunrise Manor|null|            20.729|                         0.36|\n",
      "|1772-11|Virginia Beach|null|14.138000000000002|           2.7910000000000004|\n",
      "|1908-12|    Birmingham| BHX|             9.731|                        0.862|\n",
      "|2012-05|   Baton Rouge| BTN|            25.867|          0.35100000000000003|\n",
      "+-------+--------------+----+------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from dim_temperature').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_passenger(df_immigration=df_immigration):\n",
    "    return df_immigration.select(\n",
    "    F.col('person_id'),\n",
    "    F.col('biryear'),\n",
    "    F.col('gender')).createOrReplaceTempView(\"passenger\"), spark.sql(\"create table dim_passenger as select * from passenger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have to create a passenger dimension\n",
    "df_immigration.select(\n",
    "    F.col('person_id'),\n",
    "    F.col('biryear'),\n",
    "    F.col('gender')\n",
    ").createOrReplaceTempView(\"passenger\")\n",
    "\n",
    "spark.sql(\"create table dim_passenger as select * from passenger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+\n",
      "|person_id|biryear|gender|\n",
      "+---------+-------+------+\n",
      "|        1| 1979.0|  null|\n",
      "+---------+-------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from dim_passenger').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time(df_immigration=df_immigration):\n",
    "    return df_immigration.select(\n",
    "    F.col('arrdate'),\n",
    "    F.split(F.col('arrdate'), \"-\")[0].alias('year'),\n",
    "    F.split(F.col('arrdate'), \"-\")[1].alias('month'),\n",
    "    F.split(F.col('arrdate'), \"-\")[2].alias('day'),\n",
    "    ).createOrReplaceTempView(\"time\"), spark.sql(\"create table dim_time as select * from time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have to create a time dimension\n",
    "df_immigration.select(\n",
    "    F.col('arrdate'),\n",
    "    F.split(F.col('arrdate'), \"-\")[0].alias('year'),\n",
    "    F.split(F.col('arrdate'), \"-\")[1].alias('month'),\n",
    "    F.split(F.col('arrdate'), \"-\")[2].alias('day'),\n",
    ").createOrReplaceTempView(\"time\")\n",
    "\n",
    "spark.sql(\"create table dim_time as select * from time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n",
      "|   arrdate|year|month|day|\n",
      "+----------+----+-----+---+\n",
      "|2016-04-01|2016|   04| 01|\n",
      "+----------+----+-----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from dim_time').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Here I will check if any of my tables do have rows with values.\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table fact_immigration is not empty\n",
      "table dim_temperature is not empty\n",
      "table dim_airport is not empty\n",
      "table dim_time is not empty\n",
      "table dim_passenger is not empty\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "tables = ['fact_immigration', 'dim_temperature', 'dim_airport', 'dim_time', 'dim_passenger']\n",
    "\n",
    "def check_rows(tables):\n",
    "    for i in tables:\n",
    "        if spark.sql(f'select count(*) from {i}').collect()[0][0] > 0:\n",
    "            print(f'table {i} is not empty')\n",
    "        else:\n",
    "            print(f'table {i} is empty')\n",
    "\n",
    "check_rows(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we can find all ports in dim_airpot\n",
    "\n",
    "def check_relation():\n",
    "\n",
    "    return spark.sql(\"\"\"\n",
    "\n",
    "    SELECT FA.port, AIR.location, AIR.state\n",
    "\n",
    "    FROM FACT_IMMIGRATION FA\n",
    "        INNER JOIN DIM_AIRPORT AIR ON (FA.port == AIR.code)\n",
    "\n",
    "\n",
    "    \"\"\").select('port').distinct().count() == spark.sql('select port from fact_immigration').distinct().count()\n",
    "\n",
    "check_relation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- averagetemperature: double (nullable = true)\n",
      " |-- averagetemperatureuncertainty: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dim_temperature\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|avg(averagetemperature)|\n",
      "+-----------------------+\n",
      "|      6.764667728235528|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if we can get the temperature avg in a specific port and for a specific date-month\n",
    "\n",
    "spark.sql(f'''\n",
    "\n",
    "SELECT avg(averagetemperature)\n",
    "\n",
    "FROM fact_immigration FA \n",
    "    INNER JOIN dim_temperature TP\n",
    "\n",
    "WHERE FA.PORT = \"BUF\"\n",
    "AND TP.dt = \"1821-03\"\n",
    "\n",
    "GROUP BY FA.port\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "## Fact Table schema:\n",
    "\n",
    " - |-- cicid: double (nullable = true) - Unique ID \n",
    " - |-- personid: integer (nullable = true) - FK for passenger dimension\n",
    " - |-- timeid: string (nullable = true) - FK for time dimension\n",
    " - |-- cit: double (nullable = true)\n",
    " - |-- res: double (nullable = true)\n",
    " - |-- port: string (nullable = true) - FK for airport dimension and FK for temperature dimension \n",
    " - |-- arrdate: string (nullable = true)\n",
    " - |-- mode: double (nullable = true)\n",
    " - |-- addr: string (nullable = true)\n",
    " - |-- depdate: double (nullable = true)\n",
    " - |-- bir: double (nullable = true)\n",
    " - |-- visa: double (nullable = true) - Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student)\n",
    " - |-- count: double (nullable = true)\n",
    " - |-- dtadfile: string (nullable = true)\n",
    " - |-- entdepa: string (nullable = true)\n",
    " - |-- entdepd: string (nullable = true)\n",
    " - |-- entdepu: string (nullable = true)\n",
    " - |-- matflag: string (nullable = true)\n",
    " - |-- dtaddto: string (nullable = true)\n",
    " - |-- airline: string (nullable = true) - Airline used to arrive in U. S.\n",
    " - |-- admnum: double (nullable = true)\n",
    " - |-- fltno: string (nullable = true)\n",
    " - |-- visatype: string (nullable = true) - Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    " \n",
    "\n",
    "## Dimension Tables\n",
    "\n",
    "### Airport data chema\n",
    "\n",
    " - |-- code: string (nullable = true) - Airport code Primary Key \n",
    " - |-- location: string (nullable = true) - Location \n",
    " - |-- state: string (nullable = true) - State\n",
    " \n",
    "### Person data schema:\n",
    "\n",
    " - |-- birthYear: integer (nullable = true) - 4 digit year of birth\n",
    " - |-- gender: string (nullable = true) - Gender\n",
    " - |-- personId: long (nullable = false) - Primary key\n",
    " \n",
    "### Time data schema:\n",
    "\n",
    " |-- timeid: date (nullable = true) - Primary key\n",
    " |-- month: string (nullable = true) - Month\n",
    " |-- year: string (nullable = true) - year\n",
    " |-- day: string - day\n",
    " \n",
    " ### Temperature data schema:\n",
    "\n",
    " - |-- dt: string (nullable = true) - Date and month \n",
    " - |-- city: string (nullable = true) - City \n",
    " - |-- port: string (nullable = true) - Primary key\n",
    " - |-- averagetemperature: double (nullable = true) - Temperature\n",
    " - |-- averagetemperatureuncertainty: double (nullable = true) - Temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "-- (done above)\n",
    "* Propose how often the data should be updated and why.\n",
    "-- Since immigration dataset updates montly, we can also update our data monthly\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:  \n",
    "\n",
    "\n",
    " * The data was increased by 100x.\n",
    " -- If the data is increased we can rely on cloud resources, for instance, place the data in Redshift, since it allows for querying heavy datasets.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " -- We can schedule it using airflow\n",
    " * The database needed to be accessed by 100+ people.\n",
    " -- Redshift with auto-scaling capabilities and Elastic Search for more performance of searches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:udacity_dataengineer] *",
   "language": "python",
   "name": "conda-env-udacity_dataengineer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2d0a7970c33c7ca13938fe3ac410849202128e14a82a65c75c556774fe3748f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}